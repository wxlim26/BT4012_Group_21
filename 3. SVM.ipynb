{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, make_scorer, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loading the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14254, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train data \n",
    "training_data = pd.read_csv(\"data/train.csv\")\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3564, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test data\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prepare Features and Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X_train (features), y_train (target)\n",
    "X_train = training_data.drop(['fraud'], axis = 1)\n",
    "y_train = training_data['fraud']\n",
    "\n",
    "# Prepare X_test (predictors), y_test (target)\n",
    "X_test = test_data.drop(['fraud'], axis = 1)\n",
    "y_test = test_data['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marital_status', 'high_education_ind', 'address_change_ind',\n",
       "       'past_num_of_claims', 'witness_present_ind', 'liab_prct',\n",
       "       'policy_report_filed_ind', 'age', 'safety_grade',\n",
       "       'annual_income_category', 'part_of_month', 'weekday',\n",
       "       'accident_site_Highway', 'accident_site_Local',\n",
       "       'accident_site_Parking Lot', 'channel_Broker', 'channel_Online',\n",
       "       'channel_Phone', 'claim_est_payout_category', 'age_of_vehicle_category',\n",
       "       'vehicle_price_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Baseline SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8427108180159955\n",
      "Test Accuracy: 0.8451178451178452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3012\n",
      "           1       0.00      0.00      0.00       552\n",
      "\n",
      "    accuracy                           0.85      3564\n",
      "   macro avg       0.42      0.50      0.46      3564\n",
      "weighted avg       0.71      0.85      0.77      3564\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train an SVM classifier \n",
    "svc = SVC(kernel='linear',probability=True, random_state = 0)  \n",
    "\n",
    "# Fit the model on training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate and print the accuracy score on the training data\n",
    "train_accuracy = svc.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "test_accuracy = svc.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "y_svc_pred = svc.predict_proba(X_test)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "linear_report = classification_report(y_test  , y_pred)\n",
    "\n",
    "\n",
    "# print classification report\n",
    "print(linear_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a mass imbalance in classes between Fraudulent and Non Fraudulent cases in our training data, resulting in the model struggling to correctly predict fraudulent (class 1) transactions. Hence, Synthetic Minority Over Sampling Technique (SMOTE) can be used to balance the class distribution through data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Weighting classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy after SMOTE: 0.6311912445629297\n",
      "Test Accuracy after SMOTE: 0.6262626262626263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.62      0.74      3012\n",
      "           1       0.24      0.65      0.35       552\n",
      "\n",
      "    accuracy                           0.63      3564\n",
      "   macro avg       0.57      0.64      0.54      3564\n",
      "weighted avg       0.80      0.63      0.68      3564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train an SVM classifier \n",
    "svc_2 = SVC(kernel='linear',probability=True, random_state = 0, class_weight='balanced')  \n",
    "\n",
    "# Fit the model on training data\n",
    "svc_2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate and print the accuracy score on the training data\n",
    "train_accuracy_2 = svc_2.score(X_train, y_train)\n",
    "print(\"Training Accuracy after SMOTE:\", train_accuracy_2)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "test_accuracy_2 = svc_2.score(X_test, y_test)\n",
    "print(\"Test Accuracy after SMOTE:\", test_accuracy_2)\n",
    "\n",
    "y_svc_pred_2 = svc_2.predict_proba(X_test)\n",
    "y_pred_2 = svc_2.predict(X_test)\n",
    "\n",
    "weighted_report = classification_report(y_test, y_pred_2)\n",
    "\n",
    "# print classification report\n",
    "print(weighted_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Count\n",
      "0      0  12012\n",
      "1      1   2242\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with class counts\n",
    "class_counts_df = pd.DataFrame({'Class': [0, 1], 'Count': pd.Series(y_train).value_counts().sort_index()})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(class_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24024, 21)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24024,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Count\n",
      "0      0  12012\n",
      "1      1  12012\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with class counts\n",
    "class_counts_df = pd.DataFrame({'Class': [0, 1], 'Count': pd.Series(y_train_resampled).value_counts().sort_index()})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(class_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Count\n",
      "0      0   3012\n",
      "1      1    552\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with class counts for test\n",
    "class_counts_test = pd.DataFrame({'Class': [0, 1], 'Count': pd.Series(y_test).value_counts().sort_index()})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(class_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy after SMOTE: 0.7592823842823843\n",
      "Test Accuracy after SMOTE: 0.7283950617283951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83      3012\n",
      "           1       0.26      0.39      0.31       552\n",
      "\n",
      "    accuracy                           0.73      3564\n",
      "   macro avg       0.57      0.59      0.57      3564\n",
      "weighted avg       0.78      0.73      0.75      3564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply resampled data on SVM\n",
    "# Train an SVM classifier \n",
    "svc_SMOTE = SVC(kernel='linear',probability=True, random_state = 0)  \n",
    "\n",
    "# Fit the model on training data\n",
    "svc_SMOTE.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "# Calculate and print the accuracy score on the training data\n",
    "train_accuracy_resampled = svc_SMOTE.score(X_train_resampled, y_train_resampled)\n",
    "print(\"Training Accuracy after SMOTE:\", train_accuracy_resampled)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "test_accuracy_resampled = svc_SMOTE.score(X_test, y_test)\n",
    "print(\"Test Accuracy after SMOTE:\", test_accuracy_resampled)\n",
    "\n",
    "y_svc_SMOTE_pred = svc_SMOTE.predict_proba(X_test)\n",
    "y_SMOTE_pred = svc_SMOTE.predict(X_test)\n",
    "\n",
    "SMOTE_linear_report = classification_report(y_test, y_SMOTE_pred)\n",
    "\n",
    "\n",
    "# print classification report\n",
    "print(SMOTE_linear_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a definite improvement in our precision (0.26), recall (0.39) and f1-score (0.31) after SMOTE Resampling as compared to the baseline model.\n",
    "1. Precision (for Class 1):\n",
    "Precision measures the accuracy of the positive predictions. In the context of fraud detection, precision tells you the proportion of predicted fraud cases that are actually fraud. In our model, it's 26%, meaning that out of the instances predicted as fraud, 26% are actually fraud.\n",
    "\n",
    "2. Recall (for Class 1):\n",
    "Recall (or sensitivity) measures the ability of the model to capture all the positive instances. In fraud detection, recall tells you the proportion of actual fraud cases that the model has identified. In our model, it's 39%, meaning that the model has captured 39% of the actual fraud cases.\n",
    "\n",
    "3. F1-Score (for Class 1):\n",
    "F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. A higher F1-score indicates a better trade-off between precision and recall. In our model, it's 31%, suggesting a moderate balance between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Combining SMOTE and weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 0.5933233433233434\n",
      "Class 1: 3.17885816235504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming y_train is your target variable\n",
    "class_labels = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = class_labels, y = y_train)\n",
    "\n",
    "# Print the calculated class weights\n",
    "for label, weight in zip(class_labels, class_weights):\n",
    "    print(f\"Class {label}: {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy after SMOTE: 0.7291042291042291\n",
      "Test Accuracy after SMOTE: 0.5892255892255892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.57      0.70      3012\n",
      "           1       0.23      0.69      0.34       552\n",
      "\n",
      "    accuracy                           0.59      3564\n",
      "   macro avg       0.57      0.63      0.52      3564\n",
      "weighted avg       0.80      0.59      0.65      3564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply resampled data on SVM\n",
    "# Train an SVM classifier \n",
    "svc_SMOTE = SVC(kernel='linear',probability=True, random_state = 0, class_weight = {0: 1, 1: 2})  \n",
    "\n",
    "# Fit the model on training data\n",
    "svc_SMOTE.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "# Calculate and print the accuracy score on the training data\n",
    "train_accuracy_resampled = svc_SMOTE.score(X_train_resampled, y_train_resampled)\n",
    "print(\"Training Accuracy after SMOTE:\", train_accuracy_resampled)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "test_accuracy_resampled = svc_SMOTE.score(X_test, y_test)\n",
    "print(\"Test Accuracy after SMOTE:\", test_accuracy_resampled)\n",
    "\n",
    "y_svc_SMOTE_pred = svc_SMOTE.predict_proba(X_test)\n",
    "y_SMOTE_pred = svc_SMOTE.predict(X_test)\n",
    "\n",
    "SMOTE_linear_report = classification_report(y_test, y_SMOTE_pred)\n",
    "\n",
    "\n",
    "# print classification report\n",
    "print(SMOTE_linear_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see relative improvements in the recall (0.69) and F1-score (0.34) after setting class weights to {0:1, 1:2}. We will proceed with hyper parameter tuning to find the ideal class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END ........C=0.1, class_weight=balanced, kernel=linear; total time=  20.6s\n",
      "[CV] END ........C=0.1, class_weight=balanced, kernel=linear; total time=  25.6s\n",
      "[CV] END ........C=0.1, class_weight=balanced, kernel=linear; total time=  26.2s\n",
      "[CV] END ...........C=0.1, class_weight=balanced, kernel=rbf; total time=  44.7s\n",
      "[CV] END ...........C=0.1, class_weight=balanced, kernel=rbf; total time=  49.5s\n",
      "[CV] END ...........C=0.1, class_weight=balanced, kernel=rbf; total time=  49.4s\n",
      "[CV] END ....C=0.1, class_weight={0: 1, 1: 2}, kernel=linear; total time=  21.7s\n",
      "[CV] END ....C=0.1, class_weight={0: 1, 1: 2}, kernel=linear; total time=  27.5s\n",
      "[CV] END ....C=0.1, class_weight={0: 1, 1: 2}, kernel=linear; total time=  31.8s\n",
      "[CV] END .......C=0.1, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  44.9s\n",
      "[CV] END .......C=0.1, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  48.4s\n",
      "[CV] END .......C=0.1, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  48.7s\n",
      "[CV] END ..C=0.1, class_weight={0: 1, 1: 1.5}, kernel=linear; total time=  22.9s\n",
      "[CV] END ..C=0.1, class_weight={0: 1, 1: 1.5}, kernel=linear; total time=  29.0s\n",
      "[CV] END ..C=0.1, class_weight={0: 1, 1: 1.5}, kernel=linear; total time=  29.4s\n",
      "[CV] END .....C=0.1, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  48.3s\n",
      "[CV] END .....C=0.1, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  54.8s\n",
      "[CV] END .....C=0.1, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  53.4s\n",
      "[CV] END ..........C=1, class_weight=balanced, kernel=linear; total time=  29.6s\n",
      "[CV] END ..........C=1, class_weight=balanced, kernel=linear; total time=  37.9s\n",
      "[CV] END ..........C=1, class_weight=balanced, kernel=linear; total time=  37.9s\n",
      "[CV] END .............C=1, class_weight=balanced, kernel=rbf; total time=  36.3s\n",
      "[CV] END .............C=1, class_weight=balanced, kernel=rbf; total time=  44.6s\n",
      "[CV] END .............C=1, class_weight=balanced, kernel=rbf; total time=  44.8s\n",
      "[CV] END ......C=1, class_weight={0: 1, 1: 2}, kernel=linear; total time=  32.0s\n",
      "[CV] END ......C=1, class_weight={0: 1, 1: 2}, kernel=linear; total time=  41.2s\n",
      "[CV] END ......C=1, class_weight={0: 1, 1: 2}, kernel=linear; total time=  41.1s\n",
      "[CV] END .........C=1, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  37.3s\n",
      "[CV] END .........C=1, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  45.7s\n",
      "[CV] END .........C=1, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  45.8s\n",
      "[CV] END ....C=1, class_weight={0: 1, 1: 1.5}, kernel=linear; total time=  31.1s\n",
      "[CV] END ....C=1, class_weight={0: 1, 1: 1.5}, kernel=linear; total time=  38.4s\n",
      "[CV] END ....C=1, class_weight={0: 1, 1: 1.5}, kernel=linear; total time=  39.4s\n",
      "[CV] END .......C=1, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  38.3s\n",
      "[CV] END .......C=1, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  48.1s\n",
      "[CV] END .......C=1, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  48.2s\n",
      "[CV] END .........C=10, class_weight=balanced, kernel=linear; total time= 1.2min\n",
      "[CV] END .........C=10, class_weight=balanced, kernel=linear; total time= 1.7min\n",
      "[CV] END .........C=10, class_weight=balanced, kernel=linear; total time= 1.7min\n",
      "[CV] END ............C=10, class_weight=balanced, kernel=rbf; total time=  40.2s\n",
      "[CV] END ............C=10, class_weight=balanced, kernel=rbf; total time=  53.8s\n",
      "[CV] END ............C=10, class_weight=balanced, kernel=rbf; total time=  52.7s\n",
      "[CV] END .....C=10, class_weight={0: 1, 1: 2}, kernel=linear; total time= 1.5min\n",
      "[CV] END .....C=10, class_weight={0: 1, 1: 2}, kernel=linear; total time= 2.1min\n",
      "[CV] END .....C=10, class_weight={0: 1, 1: 2}, kernel=linear; total time= 2.1min\n",
      "[CV] END ........C=10, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  39.1s\n",
      "[CV] END ........C=10, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  54.9s\n",
      "[CV] END ........C=10, class_weight={0: 1, 1: 2}, kernel=rbf; total time=  55.8s\n",
      "[CV] END ...C=10, class_weight={0: 1, 1: 1.5}, kernel=linear; total time= 1.3min\n",
      "[CV] END ...C=10, class_weight={0: 1, 1: 1.5}, kernel=linear; total time= 1.9min\n",
      "[CV] END ...C=10, class_weight={0: 1, 1: 1.5}, kernel=linear; total time= 1.9min\n",
      "[CV] END ......C=10, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  38.9s\n",
      "[CV] END ......C=10, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  53.7s\n",
      "[CV] END ......C=10, class_weight={0: 1, 1: 1.5}, kernel=rbf; total time=  55.5s\n",
      "Best Hyperparameters: {'C': 0.1, 'class_weight': {0: 1, 1: 2}, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'], #Poly omitted due to time constraints\n",
    "    'class_weight': ['balanced', {0: 1, 1: 2}, {0: 1, 1: 1.5}]\n",
    "}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svc_tune = SVC(probability=True, random_state=0)\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(svc_tune, param_grid, scoring= 'recall', cv=3, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy after tuning: 0.7252331002331003\n",
      "Test Accuracy after tuning: 0.5468574635241302\n",
      "Precision: 0.21893178212585934\n",
      "Recall: 0.75\n",
      "F1-Score: 0.3389275480966025\n"
     ]
    }
   ],
   "source": [
    "# Use the best model for predictions\n",
    "tuned_svc = grid_search.best_estimator_\n",
    "y_pred_tune = tuned_svc.predict(X_test)\n",
    "y_pred_prob_tune = tuned_svc.predict_proba(X_test)\n",
    "\n",
    "# Calculate and print the accuracy score on the training data\n",
    "train_accuracy_tune = tuned_svc.score(X_train_resampled, y_train_resampled)\n",
    "print(\"Training Accuracy after tuning:\", train_accuracy_tune)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "test_accuracy_tune = tuned_svc.score(X_test, y_test)\n",
    "print(\"Test Accuracy after tuning:\", test_accuracy_tune)\n",
    "\n",
    "# View Evaluation Metrics\n",
    "precision = precision_score(y_test, y_pred_tune, pos_label=1)\n",
    "recall = recall_score(y_test, y_pred_tune, pos_label=1)\n",
    "f1 = f1_score(y_test, y_pred_tune, pos_label=1)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned model has the hyper paramters of C = 0.1, Class_weight of 0:1, 1:2 and kernel of rbf and has the best recall while having a relatively similar precision and F1-score as the previous models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold from Youden's Index:  0.2730460791686107\n"
     ]
    }
   ],
   "source": [
    "#Youdens Index\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_tune[:,1])\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Optimal Threshold from Youden's Index: \", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.22204301075268817\n",
      "Recall: 0.7481884057971014\n",
      "F1-Score: 0.34245439469320066\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation with optimal threshold\n",
    "preds = np.where(y_pred_prob_tune[:,1] > optimal_threshold, 1, 0)\n",
    "# View Evaluation Metrics\n",
    "precision = precision_score(y_test, preds, pos_label=1)\n",
    "recall = recall_score(y_test, preds, pos_label=1)\n",
    "f1 = f1_score(y_test, preds, pos_label=1)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance out the precision and recall slightly, we can find the optimal threshold from Youden's Index, to get this final evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Precision | Recall | F1 - score |\n",
    "| --------- | ------ | ---------- | \n",
    "|   0.22    |  0.75  |    0.34   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
